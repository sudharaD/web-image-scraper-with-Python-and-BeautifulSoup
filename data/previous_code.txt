import requests
from bs4 import BeautifulSoup
import os
from urllib.parse import urljoin

class ImageDownloader():
    def __init__(self, url, output_folder, keywords, max_depth=2, max_images=150):
        self.url = url
        self.output_folder = output_folder
        self.keywords = keywords
        self.max_depth = max_depth
        self.max_images = max_images
        self.download_count = 0


    def download_images_with_keywords(self):
        """
        Downloads images from a web page based on specified keywords.

        This method initiates the download process by creating the output folder,
        setting up a visited_urls set to avoid duplicate downloads, and calling
        the internal recursive function for processing the specified URL.

        Returns:
            None
        """
        os.makedirs(self.output_folder, exist_ok=True)
        visited_urls = set()

        def download_images_internal(current_url, depth):
            """
            Recursively processes a web page, downloads images, and follows links.

            Args:
                current_url (str): The URL of the current web page.
                depth (int): The depth of the recursion, representing the level of links.

            Returns:
                None
            """
            # if depth > self.max_depth or current_url in visited_urls:
            if depth > self.max_depth or current_url in visited_urls or self.download_count >= self.max_images:
                return

            visited_urls.add(current_url)
            response = requests.get(current_url)
            soup = BeautifulSoup(response.text, 'html.parser')

            # Find images with alt attribute containing any of the keywords
            image_tags = soup.find_all('img', {'alt': lambda x: x and any(keyword.lower() in x.lower() for keyword in self.keywords)}, limit=self.max_images)
            for i, img_tag in enumerate(image_tags):
                img_url = img_tag.get('src')
                if img_url:
                    img_url = urljoin(current_url, img_url)
                    img_data = requests.get(img_url).content
                    # Construct a unique filename based on search keywords and index
                    img_filename = os.path.join(self.output_folder, f'{"_".join(self.keywords)}_{i + 1}.jpg')
                    with open(img_filename, 'wb') as img_file:
                        img_file.write(img_data)
                    print(f'Downloaded: {img_filename}')

            # Follow links recursively
            for link in soup.find_all('a', href=True):
                next_url = urljoin(current_url, link['href'])
                download_images_internal(next_url, depth + 1, count)

        download_images_internal(self.url, 0)